# -*- coding: utf-8 -*-
"""Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15O3QqT-9Fb52cl1Y9l2EWMfCThVuUh4H
"""

# from google.colab import drive
# drive.mount('/content/drive')

"""# Problem Statement

In the covid-19 pandemic period. e-commerce market is playing an important role in Global Economy. E-commerce has change the trends of purchasing methods in the community and supported the community in supplying their necessities without going outdoors and face-to-face.
 
It has become one of the most convenient way for people to purchase items conveniently and efficiently. Therefore, a churn from every customer is vital for every E-commerce company. Every churn will be a big loss to a company and because of that, when the numbers of churn customer is increasing, the company might undergo loss in their daily revenue, to be severe,they might go bankruptcy too. 
 
Due to that, we have try to solve this problem by using machine learning algorithm to create a program that helps to predict the customer that might churn on their e-commerce experience which could reduce the churn rate to prevent further losses in the future.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline,Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import matplotlib
# %matplotlib inline
import seaborn as sns
from yellowbrick.model_selection import LearningCurve
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_curve, roc_auc_score
from matplotlib.pyplot import figure

# data = pd.read_csv("/content/drive/Shareddrives/BMCS 2114 Machine Learning /E_Comm_Dataset.csv")
data = pd.read_csv("E_Comm_Dataset.csv")
data.head()

data = data.drop(columns=["CustomerID"])
data.head()

"""# **Exploratory Data Analysis**"""

data.describe().T

data.shape

data['Churn'].value_counts()

data.dtypes

X = data.drop(['Churn'], axis =1)
y = data['Churn']
X

# **Exploratory Data Analysis**graph_df = data.copy();
graph_df.Churn = graph_df.Churn.replace([0,1], ["No","Yes"])

"""# **Exploratory Data Analysis**"""



ax = plt.axes()

sns.countplot(data = graph_df, x = "Churn", palette="magma")

ax.set(xlabel='Churn',
       ylabel='Number of E-commerce Customer',
       title='Number of Customers vs Churn');

graph_categorical_col = ["PreferredLoginDevice", "PreferredPaymentMode", "Gender", "PreferedOrderCat", "MaritalStatus", "CityTier", "SatisfactionScore","Complain"]
graph_categorical_data = data[graph_categorical_col]

categorical_cols = ["PreferredLoginDevice", "PreferredPaymentMode", "Gender", "PreferedOrderCat", "MaritalStatus", "Complain"]
categorical_data = data[categorical_cols]

graph_numerical_cols = ["Tenure", "WarehouseToHome", "HourSpendOnApp", "NumberOfDeviceRegistered", "NumberOfAddress",  
                  "OrderAmountHikeFromlastYear", "CouponUsed", "OrderCount", "DaySinceLastOrder", "CashbackAmount"]
graph_numerical_data = data[graph_numerical_cols]

numerical_cols = ["Tenure", "WarehouseToHome", "HourSpendOnApp", "NumberOfDeviceRegistered", "NumberOfAddress", "CityTier", "SatisfactionScore", 
                  "OrderAmountHikeFromlastYear", "CouponUsed", "OrderCount", "DaySinceLastOrder", "CashbackAmount"]
numerical_data = data[numerical_cols]

def plot_categorical(dataset, hue, cols):
  
  fig, axes = plt.subplots(4, 2, figsize= (16,16), sharey='col')

  fig.subplots_adjust(hspace=.5)
  index = 0 
  for i in range(4):
      for j in range(2):
      
          sns.countplot(data = dataset, x = str(cols[index]) , hue = hue, ax = axes[i,j], palette="icefire_r")
          axes[i,j].grid(True)
          if(index < cols.size - 1 ):
            index +=1
  
  plt.show()

plot_categorical(graph_df,"Churn", graph_categorical_data.columns)

def plot_box_numerical(dataset, hue, cols):

  fig, axes = plt.subplots(5, 2, figsize= (16,16), sharey='col')
  sns.set_context("talk")
  sns.set_palette("dark")
  fig.subplots_adjust(hspace=.5)
  index = 0 
  for i in range(5):
    for j in range(2):
      
      sns.boxplot(data = dataset, x = str(cols[index]) , hue = hue, ax = axes[i,j], color="teal")
      axes[i,j].grid(True)
      if(index < cols.size - 1 ):
        index +=1
  

  plt.show()

plot_box_numerical(graph_df, "Churn", graph_numerical_data.columns)

matplotlib.rc_file_defaults() # to reset seaborn plotting style
sns.pairplot(data, vars = numerical_data.columns, hue = "Churn", diag_kind = 'kde', height=2.5, aspect=1)

"""# **Preprocessing**

Based on the pairplot above, we can observe that some of the numerical data columns are skewed. The log transform can be applied to solve the skewness of the data.
"""

numerical_data

numerical_data.dtypes

"""Based on the output above, the data types of numerical data in this dataset are integer and float. To allow the data store in decimal, the data types have to be changed to float."""

conv_float = ['NumberOfDeviceRegistered', 'NumberOfAddress', 'CashbackAmount', 'CityTier', 'SatisfactionScore']
numerical_data[conv_float] = numerical_data[conv_float].astype(float)
numerical_data.describe()

"""Before performing log transform onto the numerical columns data, the data with value zero have to be replaced with one to prevent error during transformation."""

add_one_col = ['Tenure','HourSpendOnApp','CouponUsed','DaySinceLastOrder','CashbackAmount']
numerical_data[add_one_col] = numerical_data[add_one_col].replace(0,1)
numerical_data.describe()

corr_mat = numerical_data.corr()

# Strip the diagonal for future examination
for x in range(corr_mat.shape[0]):
    corr_mat.iloc[x,x] = 0.0
    
corr_mat

correlation = numerical_data.corr()
fig, ax = plt.subplots(figsize=(20,15))
sns.heatmap(correlation, annot= True)

"""Based on the correlation map, we can easily observe the relationships among the numerical features in the dataset. If there exist the two features with high relationships, we can remove either one to reduce the complexity."""

def preprocess_df():
  df_eda = numerical_data
  df_eda.fillna(df_eda.mean(), inplace = True)
  print(df_eda.isnull().sum()) 
  print (corr_mat.abs().idxmax())
  log_columns = numerical_data.skew().sort_values(ascending=False)
  log_columns = log_columns.loc[log_columns > 0.75]
  print(log_columns)
  for col in log_columns.index:
    df_eda[col] = np.log1p(df_eda[col])
  df_eda = StandardScaler().fit_transform(df_eda)
  return df_eda

df_eda = preprocess_df()

y = pd.DataFrame(y, columns=['Churn'])
df_eda = pd.DataFrame(df_eda, columns = numerical_data.columns)
df_eda

df_eda = pd.concat([df_eda, y], axis = 1)
df_eda

plot_box_numerical(df_eda, "Churn", graph_numerical_data.columns)

matplotlib.rc_file_defaults() # to reset seaborn plotting style
sns.pairplot(df_eda, vars =numerical_data.columns , hue = "Churn", diag_kind = 'kde', height=2.5, aspect=1)

numerical_data = data[numerical_cols]
categorical_data = data[categorical_cols]
df = pd.concat([categorical_data, numerical_data], axis = 1)
df

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size = 0.2, random_state = 42)
X_train.head(), y_train.head()

log_transformer = FunctionTransformer(np.log1p)

numeric_median_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('log1p', log_transformer),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor_median = ColumnTransformer(
    transformers=[
        ('num', numeric_median_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)])

numeric_mean_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('log1p', log_transformer),
    ('scaler', StandardScaler())])

preprocessor_mean = ColumnTransformer(
    transformers=[
        ('num', numeric_mean_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)])

prepro_median = Pipeline(steps=[('preprocessor', preprocessor_median)])
prepro_mean = Pipeline(steps=[('preprocessor', preprocessor_mean)])

X_train_preprocessed_median = prepro_median.fit_transform(X_train)
X_test_preprocessed_median = prepro_median.transform(X_test)

X_train_preprocessed_mean = prepro_mean.fit_transform(X_train)
X_test_preprocessed_mean = prepro_mean.transform(X_test)

pd.DataFrame(X_train_preprocessed_median)

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 42)
y_train_for_median = y_train
y_train_for_mean = y_train
X_train_preprocessed_mean, y_train_for_mean = sm.fit_resample(X_train_preprocessed_mean, y_train_for_mean)
X_train_preprocessed_median, y_train_for_median = sm.fit_resample(X_train_preprocessed_median, y_train_for_median)
X_train_preprocessed_mean.shape, y_train_for_mean.shape
X_train_preprocessed_median.shape, y_train_for_median.shape

y_test.value_counts()

"""## **Modelling**"""

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

rfc = RandomForestClassifier(random_state = 42)
lgr = LogisticRegression(random_state = 42)
knn = KNeighborsClassifier(n_neighbors = 5)
dtc = DecisionTreeClassifier(random_state = 42)
svc = SVC(random_state = 42,probability=True)
mlp = MLPClassifier(max_iter=1000, random_state=42)
clf = {'LogisticRegression': lgr,  'DecisionTree': dtc, 'MultiLayerPerceptron': mlp}

def train_all_model(clf, imputed_median_set, imputed_mean_set):
    combined = [imputed_median_set, imputed_mean_set]
    accuracy_score = []

    std = []
    temp = {}
    count = 0
    for train_set in combined:
        count +=1
        accuracy_score = []
        std = []
        for key in clf.keys():
            score = cross_val_score(estimator=clf[key], X=train_set[0], y=train_set[1].values.ravel(), cv = 10)
            accuracy_score.append(score.mean())
            std.append(score.std())
#             print(score.mean())
        temp['accuracy_score'+str(count)] = accuracy_score
        temp['standard_deviation'+ str(count)] = std
    return pd.DataFrame(temp, index = clf.keys())

models_df = train_all_model(clf, [X_train_preprocessed_median, y_train_for_median], [X_train_preprocessed_mean, y_train_for_mean])
models_df

lgr_params = {
    'penalty' : ['l1', 'l2', 'elasticnet'],
    'solver' : ['newton-cg','lbfgs', 'liblinear', 'sag','saga'],
    'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]
    }
dtc_params = {
    'criterion': ['gini', 'entropy'],
    'max_leaf_nodes': list(range(2, 100)), 
    'min_samples_split': [2, 3, 4,5],
    'ccp_alpha':[0.01,0.02,0.03,0.04]
    }
mlp_params = {
    'activation' : ['tanh', 'relu'],
    'solver' : ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant', 'adaptive'],
    }
svc_params = {
    'C': [0.1,1,10,100,1000], 
    'gamma': [1,0.1,0.01,0.001,0.0001], 
    'kernel': ['rbf']
}
knn_params = {
    'n_neighbors': [x for x in range(3,50, 2)],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

classifiers = {'LogisticRegression': lgr, 'DecisionTree': dtc, 'MultiLayerPerceptron': mlp, 
               'SupportVectorMachine': svc, 'K-NearestNeighbour': knn}
params_list = [lgr_params, dtc_params, mlp_params, svc_params, knn_params]

def automated_grid_search(classifiers, params_list):
    for key , params in zip(classifiers.keys(), params_list):
        gcv = GridSearchCV(estimator = classifiers[key], param_grid = params, scoring = 'accuracy', cv = 5, n_jobs = -1, verbose = 1)
        gcv = gcv.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
        print(gcv.best_score_)
        print(gcv.best_estimator_)
        classifiers[key] =  gcv.best_estimator_
    return classifiers

# Drop unwanted columns for rfc model
categorical_cols_for_rfc = ["PreferredLoginDevice", "PreferredPaymentMode", "PreferedOrderCat", "MaritalStatus", "Complain"]
categorical_data_for_rfc = data[categorical_cols_for_rfc]
numerical_cols_for_rfc = ["Tenure", "WarehouseToHome", "NumberOfDeviceRegistered", "NumberOfAddress", "SatisfactionScore", 
                  "OrderAmountHikeFromlastYear", "DaySinceLastOrder", "CashbackAmount"]
numerical_data_for_rfc = data[numerical_cols_for_rfc]
# New dataframe for rfc model
df_for_rfc = pd.concat([categorical_data_for_rfc, numerical_data_for_rfc], axis = 1)

X_train_for_rfc, X_test_for_rfc, y_train_for_rfc, y_test_for_rfc = train_test_split(df_for_rfc, y, test_size = 0.2, random_state = 42)
# Preprocess pipeline
prepro_median_rfc = ColumnTransformer(
    transformers=[
        ('num', numeric_median_transformer, numerical_cols_for_rfc),
        ('cat', categorical_transformer, categorical_cols_for_rfc)])
X_train_preprocessed_rfc = prepro_median_rfc.fit_transform(X_train_for_rfc)
X_test_preprocessed_rfc = prepro_median_rfc.transform(X_test_for_rfc)
# SMOTE
X_train_preprocessed_rfc, y_train_for_rfc = sm.fit_resample(X_train_preprocessed_rfc, y_train_for_rfc)

rfc_params = {
    'min_samples_leaf': [1,5,10,15], 
    'n_estimators': [1,10,100,500,1000], 
    'max_features': ['auto','sqrt','log2']
}

gcv_rfc = GridSearchCV(estimator = rfc, param_grid = rfc_params, scoring = 'accuracy', cv = 5, n_jobs = -1, verbose = 1)
gcv_rfc.fit(X_train_preprocessed_rfc, y_train_for_rfc)
print(gcv_rfc.best_score_)
print(gcv_rfc.best_estimator_)
rfct = gcv_rfc.best_estimator_

tuned_classifiers = automated_grid_search(classifiers, params_list)

"""# Decision Tree"""

dtc.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
tree_pred = dtc.predict(X_test_preprocessed_median)
print(classification_report(y_test, tree_pred))
print(confusion_matrix(y_test,tree_pred))
print("Accuracy for training: {}".format(dtc.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(dtc.score(X_test_preprocessed_median, y_test)))

path = dtc.cost_complexity_pruning_path(X_train_preprocessed_median, y_train_for_median)
ccp_alphas, impurities = path.ccp_alphas, path.impurities
fig, ax = plt.subplots(figsize= (30,15))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs effective alpha for training set")

dtcs = []
for ccp_alpha in ccp_alphas:
    dtc = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    dtc.fit(X_train_preprocessed_median, y_train_for_median)
    dtcs.append(dtc)
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
      dtcs[-1].tree_.node_count, ccp_alphas[-1]))

dtcs  = dtcs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [dtc.tree_.node_count for dtc in dtcs ]
depth = [dtc.tree_.max_depth for dtc in dtcs ]
fig, ax = plt.subplots(2, 1, figsize = (15,10))
ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs alpha")
ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

train_scores = [dtc.score(X_train_preprocessed_median, y_train_for_median) for dtc in dtcs]
test_scores = [dtc.score(X_test_preprocessed_median, y_test) for dtc in dtcs]

fig, ax = plt.subplots(figsize=(15,10))
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker='o', label="train",
        drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker='o', label="test",
        drawstyle="steps-post")
ax.legend()
plt.show()

dtct = tuned_classifiers['DecisionTree']

dtct_visualizer = LearningCurve(dtct,  cv=5,scoring ='accuracy', n_jobs = -1)
dtct_visualizer.fit(X_train_preprocessed_median, y_train_for_median)
dtct_visualizer.show()

dtct.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
tree_pred = dtct.predict(X_test_preprocessed_median)
print(classification_report(y_test, tree_pred))
print(confusion_matrix(y_test,tree_pred))
print("Accuracy for training: {}".format(dtct.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing: {}\n\n".format(dtct.score(X_test_preprocessed_median, y_test)))

tree_cf = confusion_matrix(y_test,tree_pred)
names = ['True Neg', 'False Pos','False Neg','True Pos']
percentages = ['{0:.2%}'.format(value) for value in tree_cf.flatten()/np.sum(tree_cf)]
counts = ['{0:0.0f}'.format(value) for value in tree_cf.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(tree_cf, annot=labels, fmt='', cmap='Blues');

from sklearn.calibration import CalibratedClassifierCV
model_tuned = dtct
model = CalibratedClassifierCV(model_tuned)
model.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
random_probs = [0 for _ in range(len(y_test))]
model_probs = model.predict_proba(X_test_preprocessed_median)
model_probs = model_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test, random_probs)
model_auc = roc_auc_score(y_test, model_probs)

random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
model_fpr, model_tpr, _ = roc_curve(y_test, model_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(model_fpr, model_tpr, marker='.', label='Decisionn Tree prediction (AUROC= %0.3f)' %model_auc, color = 'orange')

plt.title('ROC Plot')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = dtct.predict(X_test_preprocessed_median)
model_precision, model_recall, _ = precision_recall_curve(y_test, model_probs)
model_f1 = f1_score(y_test, y_hat)
model_auc = auc(model_recall, model_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(model_recall, model_precision, marker='.', label='Decision Tree Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""# Logistic Regression"""

# Standard logistic regression
lgr.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
lgr_pred = lgr.predict(X_test_preprocessed_median)
print(classification_report(y_test, lgr_pred))
print(confusion_matrix(y_test,lgr_pred))
print("Accuracy for training: {}".format(lgr.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing: {}\n\n".format(lgr.score(X_test_preprocessed_median, y_test)))

lgrt = tuned_classifiers['LogisticRegression']
lgrt_visualizer = LearningCurve(lgrt, cv=10, scoring ='accuracy', n_jobs = -1)
lgrt_visualizer.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
lgrt_visualizer.show()

#Tuned Logistic Regression
print (lgrt)
lgrt.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
lgrt_pred = lgrt.predict(X_test_preprocessed_median)

#Confusion Matrix
lgrt_cf_matrix = confusion_matrix(y_test,lgrt_pred)
names = ['True Neg', 'False Pos','False Neg','True Pos']
percentages = ['{0:.2%}'.format(value) for value in lgrt_cf_matrix.flatten()/np.sum(lgrt_cf_matrix)]
counts = ['{0:0.0f}'.format(value) for value in lgrt_cf_matrix.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)

print('Classification Report: \n')
print(classification_report(y_test, lgrt_pred))
print("Accuracy for training: {}".format(lgrt.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(lgrt.score(X_test_preprocessed_median, y_test)))
print('\nConfusion Matrix: ')
sns.heatmap(lgrt_cf_matrix, annot=labels, fmt='', cmap='Blues');



from sklearn.calibration import CalibratedClassifierCV
model_tuned = lgrt
model = CalibratedClassifierCV(model_tuned)
model.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
random_probs = [0 for _ in range(len(y_test))]
model_probs = model.predict_proba(X_test_preprocessed_median)
model_probs = model_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test, random_probs)
model_auc = roc_auc_score(y_test, model_probs)

random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
model_fpr, model_tpr, _ = roc_curve(y_test, model_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(model_fpr, model_tpr, marker='.', label='Logistic Regression prediction (AUROC= %0.3f)' %model_auc, color = 'blue')

plt.title('ROC Plot')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = lgrt.predict(X_test_preprocessed_median)
model_precision, model_recall, _ = precision_recall_curve(y_test, model_probs)
model_f1 = f1_score(y_test, y_hat)
model_auc = auc(model_recall, model_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(model_recall, model_precision, marker='.', label='Logistic Regression Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""# Neural Networks"""

# Standard Neural Networks
mlp.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
mlp_pred = mlp.predict(X_test_preprocessed_median)
print(classification_report(y_test, mlp_pred))
print(confusion_matrix(y_test, mlp_pred))
print("Accuracy for training: {}".format(mlp.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(mlp.score(X_test_preprocessed_median, y_test)))

mlpt = tuned_classifiers['MultiLayerPerceptron']
mlpt_visualizer = LearningCurve(mlpt, cv=10, scoring ='accuracy', n_jobs = -1)
mlpt_visualizer.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
mlpt_visualizer.show()

#Tuned Neural Networks
print (mlpt)
mlpt.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
mlpt_pred = mlpt.predict(X_test_preprocessed_median)

#Confusion Matrix
mlpt_cf_matrix = confusion_matrix(y_test,mlpt_pred)
names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
percentages = ['{0:.2%}'.format(value) for value in mlpt_cf_matrix.flatten()/np.sum(mlpt_cf_matrix)]
counts = ['{0:0.0f}'.format(value) for value in mlpt_cf_matrix.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)

print('Classification Report: \n')
print(classification_report(y_test, mlpt_pred))
print("Accuracy for training: {}".format(mlpt.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(mlpt.score(X_test_preprocessed_median, y_test)))
print('\nConfusion Matrix: ')
sns.heatmap(mlpt_cf_matrix, annot=labels, fmt='', cmap='Blues');

from sklearn.calibration import CalibratedClassifierCV
model_tuned = mlpt
model = CalibratedClassifierCV(model_tuned)
model.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
random_probs = [0 for _ in range(len(y_test))]
model_probs = model.predict_proba(X_test_preprocessed_median)
model_probs = model_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test, random_probs)
model_auc = roc_auc_score(y_test, model_probs)

random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
model_fpr, model_tpr, _ = roc_curve(y_test, model_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(model_fpr, model_tpr, marker='.', label='MLP Neural Network prediction (AUROC= %0.3f)' %model_auc, color= 'green')

plt.title('ROC Plot')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = mlpt.predict(X_test_preprocessed_median)
model_precision, model_recall, _ = precision_recall_curve(y_test, model_probs)
model_f1 = f1_score(y_test, y_hat)
model_auc = auc(model_recall, model_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(model_recall, model_precision, marker='.', label='MLP Neural Network Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""# Support Vector Machine"""

from sklearn.metrics import roc_curve, precision_recall_curve
from sklearn.metrics import roc_auc_score
import scikitplot as skplt

# Standard Support Vector Machine
svc.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
svc_pred = svc.predict(X_test_preprocessed_median)
print(classification_report(y_test,svc_pred))
print(confusion_matrix(y_test,svc_pred))
print("Accuracy for training: {}".format(svc.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(svc.score(X_test_preprocessed_median, y_test)))

#if u increase gamma, the accuraccy will decrease but the processing time will be decreasing as well and vice vers
#if u increase c value, you are pressuring on the training dataset in order to classify it very well
#gamma value, if you think your model is overfit, then reduce it and vice versa, value of c as well

svct = tuned_classifiers["SupportVectorMachine"]
svct_visualizer = LearningCurve(svct, cv=10, scoring ='accuracy', n_jobs = -1)
svct_visualizer.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
svct_visualizer.show()

# Tuned Support Vector Machine
print(svct)
svct.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
svct_pred = svct.predict(X_test_preprocessed_median)

#Confusion Matrix
svct_cf_matrix = confusion_matrix(y_test,svct_pred)
names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
percentages = ['{0:.2%}'.format(value) for value in svct_cf_matrix.flatten()/np.sum(svct_cf_matrix)]
counts = ['{0:0.0f}'.format(value) for value in svct_cf_matrix.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)

print('Classification Report: \n')
print(classification_report(y_test, svct_pred))
print("Accuracy for training: {}".format(svct.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(svct.score(X_test_preprocessed_median, y_test)))
print('\nConfusion Matrix: ')
sns.heatmap(svct_cf_matrix, annot=labels, fmt='', cmap='Blues');





from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test, random_probs)
svc_auc = roc_auc_score(y_test, svc_probs)

random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
svc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(svc_fpr, svc_tpr, marker='.', label='SVC prediction (AUROC= %0.3f)' %svc_auc, color='red')

plt.title('ROC Plot')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = svct.predict(X_test_preprocessed_median)
svc_precision, svc_recall, _ = precision_recall_curve(y_test, svc_probs)
svc_f1 = f1_score(y_test, y_hat)
svc_auc = auc(svc_recall, svc_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(svc_recall, svc_precision, marker='.', label='Support Vector Machine')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""# Random Forest"""

# Standard Random Forest
rfc.fit(X_train_preprocessed_median,y_train_for_median.values.ravel())
rfc_pred = rfc.predict(X_test_preprocessed_median)
print(classification_report(y_test, rfc_pred))
print(confusion_matrix(y_test,rfc_pred))
print("Accuracy for training: {}".format(rfc.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(rfc.score(X_test_preprocessed_median, y_test)))
# print('The RFC model performance with the default parameters value is ' + str(rfc.score(X_test_preprocessed_median, y_test)))

importance = pd.DataFrame(rfc.feature_importances_)
print(importance)
preferredLoginDevice = importance[0][12] + importance[0][13] + importance[0][14]
preferredPaymentMode = importance[0][15] + importance[0][16] + importance[0][17]+ importance[0][18] + importance[0][19]+ importance[0][20]+ importance[0][21]
gender = importance[0][22] + importance[0][23]
PreferedOrderCat = importance[0][24] + importance[0][25]+ importance[0][26] + importance[0][27]+ importance[0][28]+ importance[0][29]
MaritalStatus = importance[0][30] + importance[0][31] + importance[0][32]
complainOrNot = importance[0][33] + importance[0][34]
print(preferredLoginDevice)
print(preferredPaymentMode)
print(gender)
print(PreferedOrderCat)
print(MaritalStatus)
print(complainOrNot)

rfct_visualizer = LearningCurve(rfct, cv=10, scoring ='accuracy', n_jobs = -1)
rfct_visualizer.fit(X_train_preprocessed_rfc, y_train_for_rfc.values.ravel())
rfct_visualizer.show()

#Tuned Random Forest
print (rfct)
rfct.fit(X_train_preprocessed_rfc, y_train_for_rfc.values.ravel())
rfct_pred = rfct.predict(X_test_preprocessed_rfc)

#Confusion Matrix
rfct_cf_matrix = confusion_matrix(y_test_for_rfc, rfct_pred)
names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
percentages = ['{0:.2%}'.format(value) for value in rfct_cf_matrix.flatten()/np.sum(rfct_cf_matrix)]
counts = ['{0:0.0f}'.format(value) for value in rfct_cf_matrix.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)

print('Classification Report: \n')
print(classification_report(y_test_for_rfc, rfct_pred))
print("Accuracy for training: {}".format(rfct.score(X_train_preprocessed_rfc, y_train_for_rfc)))
print("Accuracy for testing : {}".format(rfct.score(X_test_preprocessed_rfc, y_test_for_rfc)))
print('\nConfusion Matrix: ')
sns.heatmap(rfct_cf_matrix, annot=labels, fmt='', cmap='Blues');

from sklearn.calibration import CalibratedClassifierCV
model_rfc = rfct
model = CalibratedClassifierCV(model_rfc)
model.fit(X_train_preprocessed_rfc, y_train_for_rfc)
random_probs = [0 for _ in range(len(y_test_for_rfc))]
rfc_probs = model.predict_proba(X_test_preprocessed_rfc)
#svc_probs = grid.predict_proba(X_test_preprocessed)
rfc_probs = rfc_probs[:, 1]
from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test_for_rfc, random_probs)
rfc_auc = roc_auc_score(y_test_for_rfc, rfc_probs)
print('Random (chance) Prediction: AUROC = %.3f' % (random_auc))
print('Random Forestr Classifier: AUROC = %.3f' %(rfc_auc))
random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
rfc_fpr, rfc_tpr, _ = roc_curve(y_test_for_rfc, rfc_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(rfc_fpr, rfc_tpr, marker='.', label='RFC prediction (AUROC= %0.3f)' %rfc_auc, color="brown")
plt.title('ROC Plot For RFC')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = rfct.predict(X_test_preprocessed_rfc)
rfc_precision, rfc_recall, _ = precision_recall_curve(y_test_for_rfc, rfc_probs)
rfc_f1 = f1_score(y_test_for_rfc, y_hat)
rfc_auc = auc(rfc_recall, rfc_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(rfc_recall, rfc_precision, marker='.', label='Random Forest Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""# K-Nearest Neighbour

K-Nearest Neighbour Algorithm (KNN) is a non-parametric method that is used for regression and classification. KNN is also a model that helps to determine the data from Nonparametric Statistics where all the data is specified distributed but with unspecified distribution's parameter. In these cases, the input consists of the K closest training examples in the feature space (also known as the vector space); the output also depends on whether KNN is used for the classification or the regression.
 
It is also call the Lazy Algorithm as it does not need any training data points from the model generation. Throughout the training phase, all the training data will be used which makes the testing phase slower and costlier in time aspect. The function will be approximated locally and all computation is deferred until function evaluation. 

As this algorithm is dependant to distance, normalizing the training dataset able to enhance the accuracy to the models. The algorithm will be showing the performance metrics and `Accuracy, Precision, Recall, F1 score will be used to evaluate the model` in the below.


"""

knn.fit(X_train_preprocessed_median,y_train_for_median.values.ravel())
knn_pred = knn.predict(X_test_preprocessed_median)
print(classification_report(y_test, knn_pred))
print(confusion_matrix(y_test,knn_pred))
print("Accuracy for training: {}".format(rfc.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(rfc.score(X_test_preprocessed_median, y_test)))
# print('The RFC model performance with the default parameters value is ' + str(rfc.score(X_test_preprocessed_median, y_test)))

"""In KNN, K is the number of nearest neighbors and also the core deciding factor.  When K=1, then the algorithm is known as the nearest neighbor algorithm. Generally, K value needs to be an odd number so we will be setting the range from 1 to 49 and skipping all the even values. 

We will be using Mean Square Error(MSE) to get the best K value for the model.
"""

mse = []

# to determine k-value  
for i in range(1, 50, 2):
  # Create KNN Classifier
  knn = KNeighborsClassifier(n_neighbors = i)

  #Train the model using the training sets
  knn.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
    
  #Predict the response for test dataset
  y_pred_knn = knn.predict(X_test_preprocessed_median)

  y_pred_knn = y_pred_knn.reshape(1126,1)

  mean = np.mean(y_pred_knn == y_test)

  mse.append(mean)

plt.figure(figsize=(10, 10))
plt.plot(range(1, 50, 2), mse, color='blue', linewidth=1)
plt.xlabel('K')
plt.ylabel('MSE')
plt.show()

"""As the best K value above is 1, it may reduce the effect of noise on the classification, but makes the boundaries between each classification becomes more blurred.

Hence, we needs to look for the optimal K value. By finding the optimal K value, we will be calculating the accuracy score of every K value in Training Dataset and Testing Dataset

Result below shows the Accuracy for Training and Testing with k=1 until k-31
"""

neighbors = np.arange(1, 31) 
train_accuracy = np.empty(len(neighbors)) 
test_accuracy = np.empty(len(neighbors)) 
# Loop over K values 
for i, k in enumerate(neighbors): 
    knn = KNeighborsClassifier(n_neighbors=k) 
    knn.fit(X_train_preprocessed_median, y_train_for_median.values.ravel()) 
      
    # Compute traning and test data accuracy 
    train_accuracy[i] = knn.score(X_train_preprocessed_median, y_train_for_median.values.ravel()) 
    test_accuracy[i] = knn.score(X_test_preprocessed_median, y_test) 

plt.figure(figsize=(10, 10))
# Generate plot 
plt.plot(neighbors, test_accuracy, label = 'Accuracy in Testing set') 
plt.plot(neighbors, train_accuracy, label = 'Accuracy in Training set') 

plt.legend() 
plt.xlabel('K Value') 
plt.ylabel('Accuracy') 
plt.show()

"""As we can see below, as the Training Instances increasing, the Cross Validation Score increases as well. However, the Training Instances and Cross Validation Score did not have the sign of converging together. This results able to indicate that the model can insert more training exmaples in order to generalize more effectively."""

knnt = tuned_classifiers["KNearestNeighbour"]
knnt_visualizer = LearningCurve(knnt, cv=10, scoring ='accuracy', n_jobs = -1)
knnt_visualizer.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
knnt_visualizer.show()

"""The Performance Metrics used to evaluate the model with the optimal K Value (K=3) are Accuracy, Precision, Recal and F1-Score"""

# Tuned K Nearest Neighbour
print(knnt)
knnt.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
knnt_pred = knnt.predict(X_test_preprocessed_median)

#Confusion Matrix
knnt_cf_matrix = confusion_matrix(y_test,svct_pred)
names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']
percentages = ['{0:.2%}'.format(value) for value in knnt_cf_matrix.flatten()/np.sum(knnt_cf_matrix)]
counts = ['{0:0.0f}'.format(value) for value in knnt_cf_matrix.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)

print('Classification Report: \n')
print(classification_report(y_test, knnt_pred))
print("Accuracy for training: {}".format(knnt.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing : {}".format(knnt.score(X_test_preprocessed_median, y_test)))
print('\nConfusion Matrix: ')
sns.heatmap(knnt_cf_matrix, annot=labels, fmt='', cmap='Blues');

"""With a total of 1126 number of predictions made by the classifier, the results are as below:



*   True Positives (TP): 14.48% of the customer predicted will churn for E-commerce Experience
*   True Negatives (TN): 83.21% of the customers predicted will continue support the E-commerce Experience
*   False Positives (FP): 0.36% of the customers predicted to churn for E-commerce, eventually they do not. (Also known as a "Type I error.")
*   False Negatives (FN): 1.95% of the Users predicted will continue E-commercing but they have churn for the experience. (Also known as a "Type II error.")

### ROC Plot

The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. 

It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. 

The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.
"""

from sklearn.calibration import CalibratedClassifierCV
model_tuned = knnt
model = CalibratedClassifierCV(model_tuned)
model.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
random_probs = [0 for _ in range(len(y_test))]
model_probs = model.predict_proba(X_test_preprocessed_median)
model_probs = model_probs[:, 1]

from sklearn.metrics import roc_curve, roc_auc_score
#Calculate Area under ROC Curve
random_auc = roc_auc_score(y_test, random_probs)
model_auc = roc_auc_score(y_test, model_probs)

random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)
model_fpr, model_tpr, _ = roc_curve(y_test, model_probs)

import matplotlib.pyplot as plt
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.plot(model_fpr, model_tpr, marker='.', label='KNN prediction (AUROC= %0.3f)' %model_auc, color= 'purple')

plt.title('ROC Plot')
#Axis Lables
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

"""It is evident from the plot that the AUC for the KNN ROC curve with the score of 0.975. Therefore, we can say that K-Nearest Neighbors did quite a good job of classifying the positive class in the dataset."""

from sklearn.metrics import f1_score
from sklearn.metrics import auc
y_hat = knnt.predict(X_test_preprocessed_median)
model_precision, model_recall, _ = precision_recall_curve(y_test, model_probs)
model_f1 = f1_score(y_test, y_hat)
model_auc = auc(model_recall, model_precision)
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.plot(model_recall, model_precision, marker='.', label='KNN Classifier')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()
#Show plot
plt.show()

"""As the result shown above, when our precision score is 1.0 which indicates no false positive, our recall will be very low because we still have many false negatives. This results remain after the Recall score is more than 0.9, the Precision Score start decreasing rapidly.

# Training Accuracy Comparison Based on All Models
"""

classifier_for_compare = tuned_classifiers
# rfct

classifier_list = [] 
for classifier in classifier_for_compare.keys():
    temp = {}
    temp["Classifier"] = classifier
    temp["Training Accuracy"] = classifier_for_compare[classifier].score(X_train_preprocessed_median, y_train_for_median)
    temp["Testing Accuracy"] = classifier_for_compare[classifier].score(X_test_preprocessed_median, y_test)
    classifier_list.append(temp)

rfc_compare = {'Classifier': 'RandomForestClassifer', 
               'Training Accuracy': rfct.score(X_train_preprocessed_rfc, y_train_for_rfc),
               'Testing Accuracy': rfct.score(X_test_preprocessed_rfc, y_test_for_rfc)}

classifier_list.append(rfc_compare)
class_df = pd.DataFrame.from_dict(classifier_list, orient='columns')

plt.subplots(figsize= (15,10))
sns.barplot(data = class_df, x = 'Training Accuracy', y = 'Classifier', orient = 'h')
plt.title("Compare Model Training Accuracy", size = 20)
plt.xlabel(xlabel='Training Accuracy',size = 20)
plt.ylabel(ylabel='Classifier' ,size = 20)
plt.grid()

plt.subplots(figsize= (15,10))
sns.barplot(data = class_df, x = 'Testing Accuracy', y = 'Classifier', orient = 'h')
plt.title("Compare Model Testing Accuracy", size = 20)
plt.xlabel(xlabel='Testing Accuracy',size = 20)
plt.ylabel(ylabel='Classifier' ,size = 20)
plt.grid()

"""# Ensemble Method"""

from sklearn.ensemble import VotingClassifier

#create a dictionary of our models
estimators=[('mlp', mlpt), ('rfc', rfct), ('svm', svct)]
#create our voting classifier, inputting our models
ensemble = VotingClassifier(estimators, voting='hard')

ensemble.fit(X_train_preprocessed_median, y_train_for_median.values.ravel())
pred_ensemble = ensemble.predict(X_test_preprocessed_median)



print(classification_report(y_test, pred_ensemble))
ensemble_cf = confusion_matrix(y_test, pred_ensemble)
names = ['True Neg', 'False Pos','False Neg','True Pos']
percentages = ['{0:.2%}'.format(value) for value in ensemble_cf.flatten()/np.sum(ensemble_cf)]
counts = ['{0:0.0f}'.format(value) for value in ensemble_cf.flatten()]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in zip(names, percentages, counts)]
labels = np.asarray(labels).reshape(2,2)
print("Accuracy for training: {}".format(ensemble.score(X_train_preprocessed_median, y_train_for_median)))
print("Accuracy for testing: {}\n\n".format(ensemble.score(X_test_preprocessed_median, y_test)))
sns.heatmap(ensemble_cf, annot=labels, fmt='', cmap='Blues');

"""# ROC Curve"""

model_probs_list = []
def plot_all_model_roc_curve(tuned_classifiers, imputed_median_set, test_set):
        for classifier in tuned_classifiers.keys():
            model = tuned_classifiers[classifier]
            model_tuned = CalibratedClassifierCV(model)
            model_tuned.fit(imputed_median_set[0], imputed_median_set[1].values.ravel())
            random_probs = [0 for _ in range(len(test_set[1]))]
            model_probs = model_tuned.predict_proba(test_set[0])
            model_probs = model_probs[:, 1]
            model_probs_list.append(model_probs)
            random_auc = roc_auc_score(test_set[1], random_probs)
            model_auc = roc_auc_score(test_set[1], model_probs)
            print('Random (chance) Prediction: AUROC = %.3f' % (random_auc))
            print( classifier + ': AUROC = %.3f' %(model_auc))
            random_fpr, random_tpr, _ = roc_curve(test_set[1], random_probs)
            model_fpr, model_tpr, _ = roc_curve(test_set[1], model_probs)
            plt.plot(model_fpr, model_tpr, marker='_', label='RFC prediction for ' + classifier + '(AUROC= %0.3f)' %model_auc)
            plt.title('ROC Plot For Classifier with best param')

def plot_rfc_roc_curve(rfct, imputed_median_set, test_set):
    model = rfct
    model_tuned = CalibratedClassifierCV(model)
    model_tuned.fit(imputed_median_set[0], imputed_median_set[1].values.ravel())
    random_probs = [0 for _ in range(len(test_set[1]))]
    model_probs = model_tuned.predict_proba(test_set[0])
    model_probs = model_probs[:, 1]
    model_probs_list.append(model_probs)
    random_auc = roc_auc_score(test_set[1], random_probs)
    model_auc = roc_auc_score(test_set[1], model_probs)
    print('Random (chance) Prediction: AUROC = %.3f' % (random_auc))
    print('Tuned Random Forest Classifier : AUROC = %.3f' %(model_auc))
    random_fpr, random_tpr, _ = roc_curve(test_set[1], random_probs)
    model_fpr, model_tpr, _ = roc_curve(test_set[1], model_probs)
    plt.plot(model_fpr, model_tpr, marker='_', label='RFC prediction for Random Forest Classifier(AUROC= %0.3f)' %model_auc)

figure(figsize=(8, 6), dpi=80)
plot_all_model_roc_curve(tuned_classifiers, [X_train_preprocessed_median, y_train_for_median], [X_test_preprocessed_median,y_test])
plot_rfc_roc_curve(rfct,[X_train_preprocessed_median, y_train_for_median], [X_test_preprocessed_median,y_test])
#Axis Lables
plt.plot(random_fpr, random_tpr,linestyle='--', label='Random prediction (AUROC= %0.3f)' %random_auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
#Show Legend
plt.legend()
#Show plot
plt.show()

from sklearn.metrics import f1_score
from sklearn.metrics import auc
plt.subplots(figsize= (20,10))
def plot_models_precision_recall_curve(tuned_classifiers, imputed_median_set, test_set):
    i = 0
    for classifier in tuned_classifiers.keys():
        y_hat = tuned_classifiers[classifier].predict(X_test_preprocessed_median)
        model_precision, model_recall, _ = precision_recall_curve(y_test, model_probs_list[i])
        model_f1 = f1_score(y_test, y_hat)
        model_auc = auc(model_recall, model_precision)

        plt.plot(model_recall, model_precision, marker='.', label=classifier)
        i+=1
plot_models_precision_recall_curve(tuned_classifiers, [X_train_preprocessed_median, y_train_for_median], [X_test_preprocessed_median,y_test])
y_hat = rfct.predict(X_test_preprocessed_rfc)
rfc_precision, rfc_recall, _ = precision_recall_curve(y_test, rfc_probs)
rfc_f1 = f1_score(y_test, y_hat)
rfc_auc = auc(rfc_recall, rfc_precision)
plt.plot(rfc_recall, rfc_precision, marker='.', label='Random Forest Classifier')
no_skill = 185/(941+185)
plt.plot([0,1], [no_skill, no_skill], linestyle='--', label= 'baseline')
plt.title('Precision and Recall Curve for Models')
plt.xlabel('Recall')
plt.ylabel('Precision')
#Show the legend
plt.legend()

#Show plot
plt.show()

"""# Save Model"""

# import pickle

# for classifier in tuned_classifiers.keys():
#     print(tuned_classifiers[classifier])
#     Pkl_Filename = classifier + "_Model.pkl"
#     with open(Pkl_Filename, 'wb') as file:  
#         pickle.dump(tuned_classifiers[classifier], file)
        
# rfc_file_name = "RandomForestClassifier.pkl"

# with open(rfc_file_name, 'wb') as file:  
#         pickle.dump(rfct, file)
        
# ensemble_file_name = "ensemble.pkl"

# with open(ensemble_file_name, 'wb') as file:  
#         pickle.dump(ensemble, file)

# load_classifier = {}

# for classifier in tuned_classifiers.keys():
#     Pkl_Filename = classifier + "_Model.pkl"
#     with open(Pkl_Filename, 'rb') as file:  
#         Pickled_Model = pickle.load(file)
#         load_classifier[classifier] = Pickled_Model
#         print(classifier, ":",Pickled_Model.score(X_test_preprocessed_median, y_test))
        
# rfc_file_name = "RandomForestClassifier.pkl"

# with open(rfc_file_name, 'rb') as file:  
#         Pickled_Model = pickle.load(file)
#         print("Random Forest Classifier: ",Pickled_Model.score(X_test_preprocessed_rfc, y_test))
# load_classifier['RandomForest'] = Pickled_Model  

# ensemble_file_name = "ensemble.pkl"

# with open(ensemble_file_name, 'rb') as file:  
#         Pickled_Model = pickle.load(file)
#         print("Ensemble Classifier: ", Pickled_Model.score(X_test_preprocessed_median, y_test))
        
        
# load_classifier['Ensemble'] =  Pickled_Model

# print(load_classifier)

